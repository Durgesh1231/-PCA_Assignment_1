{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. What is the curse of dimensionality reduction and why is it important in machine learning?\n",
        "# No specific code, this is more about understanding.\n",
        "# But here's an example of how high-dimensional data increases sparsity:\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic high-dimensional dataset\n",
        "X, y = make_classification(n_samples=100, n_features=50, n_informative=10, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Plot the data in 2D to show sparsity\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "plt.title('High-dimensional data visualized in 2D')\n",
        "plt.show()\n",
        "\n",
        "# Q2. How does the curse of dimensionality impact the performance of machine learning algorithms?\n",
        "# Example of using KNN classifier on high-dimensional data and seeing the effect on accuracy\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic dataset with many features\n",
        "X, y = make_classification(n_samples=100, n_features=50, n_informative=10, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = knn.predict(X_test)\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "\n",
        "# Q3. What are some of the consequences of the curse of dimensionality in machine learning, and how do they impact model performance?\n",
        "# Overfitting example with high-dimensional data in KNN\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=50, n_informative=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Use KNN with a low number of neighbors to simulate overfitting\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = knn.predict(X_test)\n",
        "print(f'Accuracy with Overfitting (n_neighbors=1): {accuracy_score(y_test, y_pred)}')\n",
        "\n",
        "# Q4. Can you explain the concept of feature selection and how it can help with dimensionality reduction?\n",
        "\n",
        "# Example using Recursive Feature Elimination (RFE) for feature selection\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Create the base model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Select features using RFE\n",
        "selector = RFE(model, n_features_to_select=2)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "print(f'Selected features: {selector.support_}')\n",
        "\n",
        "# Q5. What are some limitations and drawbacks of using dimensionality reduction techniques in machine learning?\n",
        "\n",
        "# Example: PCA might lose important features\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Apply PCA to reduce dimensions to 2\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Train a KNN model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy after dimensionality reduction\n",
        "y_pred = knn.predict(X_test)\n",
        "print(f'Accuracy with PCA: {accuracy_score(y_test, y_pred)}')\n",
        "\n",
        "# Q6. How does the curse of dimensionality relate to overfitting and underfitting in machine learning?\n",
        "\n",
        "# Example: High-dimensional data might lead to overfitting and underfitting with KNN\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate high-dimensional data\n",
        "X, y = make_classification(n_samples=100, n_features=50, random_state=42)\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Overfitting case (low k value)\n",
        "knn_overfit = KNeighborsClassifier(n_neighbors=1)\n",
        "knn_overfit.fit(X_train, y_train)\n",
        "y_pred_overfit = knn_overfit.predict(X_test)\n",
        "print(f'Accuracy (Overfitting): {accuracy_score(y_test, y_pred_overfit)}')\n",
        "\n",
        "# Underfitting case (high k value)\n",
        "knn_underfit = KNeighborsClassifier(n_neighbors=50)\n",
        "knn_underfit.fit(X_train, y_train)\n",
        "y_pred_underfit = knn_underfit.predict(X_test)\n",
        "print(f'Accuracy (Underfitting): {accuracy_score(y_test, y_pred_underfit)}')\n",
        "\n",
        "# Q7. How can one determine the optimal number of dimensions to reduce data to when using dimensionality reduction techniques?\n",
        "\n",
        "# Using explained variance to determine the optimal number of dimensions with PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Plot the explained variance ratio\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance vs Number of Components')\n",
        "plt.show()\n"
      ]
    }
  ]
}